{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d55b163",
   "metadata": {},
   "source": [
    "# 1) Scrape book titles and prices from the first 2 pages of Books to Scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50ee3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "base_url = \"http://books.toscrape.com/catalogue/page-{}.html\"\n",
    "books = []\n",
    "for page in [1, 2]:\n",
    "    resp = requests.get(base_url.format(page))\n",
    "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "    for prod in soup.select(\"article.product_pod\"):\n",
    "        title = prod.h3.a[\"title\"]\n",
    "        price = prod.select_one(\"p.price_color\").text.strip()\n",
    "        books.append({\"title\": title, \"price\": price})\n",
    "books\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd7ecee",
   "metadata": {},
   "source": [
    "# 2) Extract weather descriptions and temperatures for 5 cities from wttr.in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48425630",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "cities = [\"Mumbai\", \"Delhi\", \"London\", \"New York\", \"Tokyo\"]\n",
    "weather_data = []\n",
    "for city in cities:\n",
    "    url = f\"https://wttr.in/{city}?format=%l:+%C+%t\"\n",
    "    r = requests.get(url)\n",
    "    raw = r.text.strip()\n",
    "    if \": \" in raw:\n",
    "        cityname, rest = raw.split(\": \", 1)\n",
    "        parts = rest.split(\" \", 1)\n",
    "        condition = parts[0]\n",
    "        temp = parts[1] if len(parts) > 1 else \"\"\n",
    "    else:\n",
    "        cityname, condition, temp = city, \"\", \"\"\n",
    "    weather_data.append({\"city\": cityname, \"description\": condition, \"temperature\": temp})\n",
    "\n",
    "weather_df = pd.DataFrame(weather_data)\n",
    "weather_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7573969",
   "metadata": {},
   "source": [
    "# 3) Scrape Fake Jobs Board (first 3 pages) and save to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae70e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import csv\n",
    "\n",
    "jobs = []\n",
    "for page in range(1, 4):\n",
    "    url = f\"https://realpython.github.io/fake-jobs/?page={page - 1}\"\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "    for card in soup.select(\"div.card-content\"):\n",
    "        title = card.select_one(\"h2.title\").text.strip()\n",
    "        company = card.select_one(\"h3.company\").text.strip()\n",
    "        location = card.select_one(\"p.location\").text.strip()\n",
    "        jobs.append({\"title\": title, \"company\": company, \"location\": location})\n",
    "\n",
    "csv_filename = \"fake_jobs_first_three_pages.csv\"\n",
    "with open(csv_filename, \"w\", newline='', encoding=\"utf-8\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=[\"title\", \"company\", \"location\"])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(jobs)\n",
    "\n",
    "len(jobs), csv_filename\n"
   ]
  }
 ],
 "metadata": {
  "language": "python"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}